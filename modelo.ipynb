{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@misc{home-credit-credit-risk-model-stability,\\n    author = {Daniel Herman, Tomas Jelinek, Walter Reade, Maggie Demkin, Addison Howard},\\n    title = {Home Credit - Credit Risk Model Stability},\\n    publisher = {Kaggle},\\n    year = {2024},\\n    url = {https://kaggle.com/competitions/home-credit-credit-risk-model-stability}\\n}\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@misc{home-credit-credit-risk-model-stability,\n",
    "    author = {Daniel Herman, Tomas Jelinek, Walter Reade, Maggie Demkin, Addison Howard},\n",
    "    title = {Home Credit - Credit Risk Model Stability},\n",
    "    publisher = {Kaggle},\n",
    "    year = {2024},\n",
    "    url = {https://kaggle.com/competitions/home-credit-credit-risk-model-stability}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHome Credit - Credit Risk Model Stability\\nCreate a model measured against feature stability over time\\nThe goal of this competition is to predict which clients are more likely to default on their loans. \\nThe evaluation will favor solutions that are stable over time.\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Home Credit - Credit Risk Model Stability\n",
    "Create a model measured against feature stability over time\n",
    "The goal of this competition is to predict which clients are more likely to default on their loans. \n",
    "The evaluation will favor solutions that are stable over time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLack of credit history could indicate youth or a preference for using cash.\\nTraditional data scarcity often leads to loan application rejections.\\nData science is essential in determining loan repayment capabilities, potentially making loans more accessible to those in need.\\nConsumer finance providers utilize various statistical and machine learning methods in scorecards to assess loan risk.\\nScorecards must be updated regularly due to the constant change in client behavior, making the process time-consuming.\\nThe stability of a scorecard is crucial; a performance decline could mean issuing loans to higher-risk clients.\\nEarly detection of loan repayment issues is challenging and usually visible only after loan due dates.\\nBalancing the stability and performance of scorecards is necessary before their implementation.\\nHome Credit, established in 1997, aims to offer responsible lending to people without significant credit history.\\nHome Credit's efforts in financial inclusion have previously been showcased in a Kaggle competition.\\nImproving scorecard assessments could help consumer finance providers accept more loan applications, enhancing financial access for historically underserved groups.\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lack of credit history could indicate youth or a preference for using cash.\n",
    "Traditional data scarcity often leads to loan application rejections.\n",
    "Data science is essential in determining loan repayment capabilities, potentially making loans more accessible to those in need.\n",
    "Consumer finance providers utilize various statistical and machine learning methods in scorecards to assess loan risk.\n",
    "Scorecards must be updated regularly due to the constant change in client behavior, making the process time-consuming.\n",
    "The stability of a scorecard is crucial; a performance decline could mean issuing loans to higher-risk clients.\n",
    "Early detection of loan repayment issues is challenging and usually visible only after loan due dates.\n",
    "Balancing the stability and performance of scorecards is necessary before their implementation.\n",
    "Home Credit, established in 1997, aims to offer responsible lending to people without significant credit history.\n",
    "Home Credit's efforts in financial inclusion have previously been showcased in a Kaggle competition.\n",
    "Improving scorecard assessments could help consumer finance providers accept more loan applications, enhancing financial access for historically underserved groups.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor each case_id in the test set, you must predict a probability for the target score. The file should contain a header and have the following format:\\n\\ncase_id,score\\n57543,0.1\\n57544,0.9\\n57545,0.5\\netc.\\n\\n\\nSubmission file must be named submission.csv\\n\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each case_id in the test set, you must predict a probability for the target score. The file should contain a header and have the following format:\n",
    "\n",
    "case_id,score\n",
    "57543,0.1\n",
    "57544,0.9\n",
    "57545,0.5\n",
    "etc.\n",
    "\n",
    "\n",
    "Submission file must be named submission.csv\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n predicting default of clients based on internal and external information that are available for each client. Scoring is \\n performed using custom metric that not only evaluates the AUC of predictions but also considers the stability of predictions \\n model across the data range of the test set.\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " predicting default of clients based on internal and external information that are available for each client. Scoring is \n",
    " performed using custom metric that not only evaluates the AUC of predictions but also considers the stability of predictions \n",
    " model across the data range of the test set.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl #Libreria para manipulacion de datos, fast wrote in rust https://docs.pola.rs \n",
    "import numpy as np #Libreria para manipulacion de datos \n",
    "import pandas as pd #Libreria para manipulacion de datos \n",
    "import lightgbm as lgb #Libreria para modelos de machine learning,  is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cambia la ruta de acuerdo a la ubicacion de los datos\n",
    "dataPath = \"C:/Users/diego/Documents/UP/Negocios/DataProyecto/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establecer_tipos_de_datos_tabla(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # implementar aquí todos los tipos de datos deseados para las tablas\n",
    "    # lo siguiente es solo un ejemplo\n",
    "    for columna in df.columns:\n",
    "        # la última letra del nombre de la columna te ayudará a determinar el tipo\n",
    "        if columna[-1] in (\"P\", \"A\"):\n",
    "            df = df.with_columns(pl.col(columna).cast(pl.Float64).alias(columna)) #Pasa las columnas a float64\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_cadenas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for columna in df.columns:  \n",
    "        if df[columna].dtype.name in ['object', 'string']:\n",
    "            df[columna] = df[columna].astype(\"string\").astype('category')\n",
    "            categorias_actuales = df[columna].cat.categories\n",
    "            nuevas_categorias = categorias_actuales.to_list() + [\"Desconocido\"]\n",
    "            nuevo_tipo = pd.CategoricalDtype(categories=nuevas_categorias, ordered=True)\n",
    "            df[columna] = df[columna].astype(nuevo_tipo)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_base = dataPath + \"csv_files/train/\"\n",
    "\n",
    "# Leer el archivo base de entrenamiento\n",
    "tabla_base_entrenamiento = pl.read_csv(ruta_base + \"train_base.csv\")\n",
    "\n",
    "# Leer y concatenar los archivos estáticos de entrenamiento\n",
    "archivo_estatico_1 = pl.read_csv(ruta_base + \"train_static_0_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "archivo_estatico_2 = pl.read_csv(ruta_base + \"train_static_0_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_static = pl.concat([archivo_estatico_1, archivo_estatico_2], how=\"vertical_relaxed\")\n",
    "\n",
    "# Leer otros archivos de entrenamiento\n",
    "entrenamiento_estatico_cb = pl.read_csv(ruta_base + \"train_static_cb_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_person_1 = pl.read_csv(ruta_base + \"train_person_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_credit_bureau_b_2 = pl.read_csv(ruta_base + \"train_credit_bureau_b_2.csv\").pipe(establecer_tipos_de_datos_tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_base_prueba = dataPath + \"csv_files/test/\"\n",
    "\n",
    "# Leer el archivo base de prueba\n",
    "train_basetable = pl.read_csv(ruta_base_prueba + \"test_base.csv\")\n",
    "\n",
    "# Leer y concatenar los archivos estáticos de prueba\n",
    "archivo_estatico_prueba_1 = pl.read_csv(ruta_base_prueba + \"test_static_0_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "archivo_estatico_prueba_2 = pl.read_csv(ruta_base_prueba + \"test_static_0_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "archivo_estatico_prueba_3 = pl.read_csv(ruta_base_prueba + \"test_static_0_2.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "prueba_estatica = pl.concat([archivo_estatico_prueba_1, archivo_estatico_prueba_2, archivo_estatico_prueba_3], how=\"vertical_relaxed\")\n",
    "\n",
    "# Leer otros archivos de prueba\n",
    "train_static_cb = pl.read_csv(ruta_base_prueba + \"test_static_cb_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "test_person_1 = pl.read_csv(ruta_base_prueba + \"test_person_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "test_credit_bureau_b_2 = pl.read_csv(ruta_base_prueba + \"test_credit_bureau_b_2.csv\").pipe(establecer_tipos_de_datos_tabla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A']\n",
      "['description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n",
      "Data: /n\n",
      "shape: (10, 57)\n",
      "┌─────────┬────────────┬────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
      "│ case_id ┆ date_decis ┆ MONTH  ┆ WEEK_NUM ┆ … ┆ alguna_vez ┆ tipo_casa_ ┆ maximo_pmt ┆ alguna_vez │\n",
      "│ ---     ┆ ion        ┆ ---    ┆ ---      ┆   ┆ _selfemplo ┆ persona    ┆ s_pmtsover ┆ _pmts_dpdv │\n",
      "│ i64     ┆ ---        ┆ i64    ┆ i64      ┆   ┆ yed        ┆ ---        ┆ due_635A   ┆ alue_108P_ │\n",
      "│         ┆ str        ┆        ┆          ┆   ┆ ---        ┆ str        ┆ ---        ┆ …          │\n",
      "│         ┆            ┆        ┆          ┆   ┆ bool       ┆            ┆ f64        ┆ ---        │\n",
      "│         ┆            ┆        ┆          ┆   ┆            ┆            ┆            ┆ bool       │\n",
      "╞═════════╪════════════╪════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
      "│ 57543   ┆ 2021-05-14 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57549   ┆ 2022-01-17 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57551   ┆ 2020-11-27 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57552   ┆ 2020-11-27 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57569   ┆ 2021-12-20 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57630   ┆ 2021-03-16 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57631   ┆ 2022-06-04 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57632   ┆ 2022-02-05 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57633   ┆ 2022-01-25 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "│ 57634   ┆ 2021-01-27 ┆ 202201 ┆ 100      ┆ … ┆ null       ┆ null       ┆ null       ┆ null       │\n",
      "└─────────┴────────────┴────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Necesitamos usar funciones de agregación en tablas con profundidad > 1, es decir, tablas que contienen la columna num_group1 o\n",
    "# también la columna num_group2.\n",
    "caracteristicas_persona_1 = train_person_1.group_by(\"case_id\").agg(\n",
    "    # Obtenemos el valor máximo de la columna 'mainoccupationinc_384A' para cada 'case_id'\n",
    "    pl.col(\"mainoccupationinc_384A\").max().alias(\"maximo_mainoccupationinc_384A\"),\n",
    "    # Verificamos si alguna vez 'incometype_1044T' es 'SELFEMPLOYED' para cada 'case_id'\n",
    "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"alguna_vez_selfemployed\")\n",
    ")\n",
    "\n",
    "# Aquí num_group1=0 tiene un significado especial, es la persona que solicitó el préstamo.\n",
    "caracteristicas_persona_2 = train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
    "    # Filtramos las filas donde 'num_group1' es 0\n",
    "    pl.col(\"num_group1\") == 0\n",
    "# Eliminamos la columna 'num_group1' y renombramos la columna 'housetype_905L' a 'tipo_casa_persona'\n",
    ").drop(\"num_group1\").rename({\"housetype_905L\": \"tipo_casa_persona\"})\n",
    "\n",
    "# Aquí tenemos num_goup1 y num_group2, por lo que necesitamos agregar de nuevo.\n",
    "caracteristicas_credito_b_2 = train_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
    "    # Obtenemos el valor máximo de la columna 'pmts_pmtsoverdue_635A' para cada 'case_id'\n",
    "    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"maximo_pmts_pmtsoverdue_635A\"),\n",
    "    # Verificamos si alguna vez 'pmts_dpdvalue_108P' es mayor que 31 para cada 'case_id'\n",
    "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"alguna_vez_pmts_dpdvalue_108P_mayor_31\")\n",
    ")\n",
    "\n",
    "# Procesaremos en este ejemplo solo las columnas de tipo A y M, por lo que necesitamos seleccionarlas.\n",
    "columnas_estaticas_seleccionadas = [col for col in train_static.columns if col[-1] in (\"A\", \"M\")]\n",
    "print(columnas_estaticas_seleccionadas)\n",
    "\n",
    "columnas_estaticas_cb_seleccionadas = [col for col in train_static_cb.columns if col[-1] in (\"A\", \"M\")]\n",
    "print(columnas_estaticas_cb_seleccionadas)\n",
    "\n",
    "# Unimos todas las tablas.\n",
    "data = train_basetable.join(\n",
    "    train_static.select([\"case_id\"]+columnas_estaticas_seleccionadas), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    train_static_cb.select([\"case_id\"]+columnas_estaticas_cb_seleccionadas), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    caracteristicas_persona_1, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    caracteristicas_persona_2, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    caracteristicas_credito_b_2, how=\"left\", on=\"case_id\"\n",
    ")\n",
    "\n",
    "print(\"Data: /n\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos usar funciones de agregación en tablas con profundidad > 1, es decir, tablas que contienen la columna num_group1 o\n",
    "# también la columna num_group2.\n",
    "test_person_1_feats_1 = test_person_1.group_by(\"case_id\").agg(\n",
    "    # Obtenemos el valor máximo de la columna 'mainoccupationinc_384A' para cada 'case_id'\n",
    "    pl.col(\"mainoccupationinc_384A\").max().alias(\"maximo_mainoccupationinc_384A\"),\n",
    "    # Verificamos si alguna vez 'incometype_1044T' es 'SELFEMPLOYED' para cada 'case_id'\n",
    "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"alguna_vez_selfemployed\")\n",
    ")\n",
    "\n",
    "# Aquí num_group1=0 tiene un significado especial, es la persona que solicitó el préstamo.\n",
    "test_person_1_feats_2 = test_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
    "    # Filtramos las filas donde 'num_group1' es 0\n",
    "    pl.col(\"num_group1\") == 0\n",
    "# Eliminamos la columna 'num_group1' y renombramos la columna 'housetype_905L' a 'tipo_casa_persona'\n",
    ").drop(\"num_group1\").rename({\"housetype_905L\": \"tipo_casa_persona\"})\n",
    "\n",
    "# Aquí tenemos num_goup1 y num_group2, por lo que necesitamos agregar de nuevo.\n",
    "test_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
    "    # Obtenemos el valor máximo de la columna 'pmts_pmtsoverdue_635A' para cada 'case_id'\n",
    "    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"maximo_pmts_pmtsoverdue_635A\"),\n",
    "    # Verificamos si alguna vez 'pmts_dpdvalue_108P' es mayor que 31 para cada 'case_id'\n",
    "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"alguna_vez_pmts_dpdvalue_108P_mayor_31\")\n",
    ")\n",
    "\n",
    "# Unimos todas las tablas.\n",
    "data_submission = train_basetable.join(\n",
    "    prueba_estatica.select([\"case_id\"]+columnas_estaticas_seleccionadas), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    train_static_cb.select([\"case_id\"]+columnas_estaticas_cb_seleccionadas), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A', 'maximo_mainoccupationinc_384A', 'maximo_pmts_pmtsoverdue_635A']\n"
     ]
    }
   ],
   "source": [
    "# Primero, obtenemos los 'case_id' únicos y los mezclamos.\n",
    "case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
    "\n",
    "# Luego, dividimos los 'case_id' en conjuntos de entrenamiento y prueba.\n",
    "case_ids_train, case_ids_test = train_test_split(case_ids.to_numpy(), train_size=0.6, random_state=1) \n",
    "# Dividimos el conjunto de prueba en conjuntos de validación y prueba.\n",
    "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
    "\n",
    "# Creamos una lista vacía para almacenar las columnas de predicción.\n",
    "cols_pred = []\n",
    "# Recorremos todas las columnas en los datos.\n",
    "for col in data.columns:\n",
    "    # Si el último carácter de la columna es una letra mayúscula y el resto de la columna es minúscula,\n",
    "    # añadimos la columna a la lista de columnas de predicción.\n",
    "    if col[-1].isupper() and col[:-1].islower():\n",
    "        cols_pred.append(col)\n",
    "\n",
    "# Imprimimos las columnas de predicción.\n",
    "print(cols_pred)\n",
    "\n",
    "\n",
    "# Definimos una función para convertir los datos de Polars a Pandas.\n",
    "def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Filtramos los datos por 'case_id' y seleccionamos las columnas 'case_id', 'WEEK_NUM' y 'target'.\n",
    "    # Convertimos los datos a Pandas.\n",
    "    return (\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"date_decision\"]].to_pandas(),\n",
    "        # Filtramos los datos por 'case_id' y seleccionamos las columnas de predicción.\n",
    "        # Convertimos los datos a Pandas.\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
    "        # Filtramos los datos por 'case_id' y seleccionamos la columna 'target'.\n",
    "        # Convertimos los datos a Pandas.\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"date_decision\"].to_pandas()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\diego\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (16.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\diego\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyarrow) (1.26.4)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "pa.Table requires 'pyarrow' module to be installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Usamos la función definida anteriormente para convertir los conjuntos de entrenamiento, validación y prueba a Pandas.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m base_train, X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_polars_to_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase_ids_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m base_valid, X_valid, y_valid \u001b[38;5;241m=\u001b[39m from_polars_to_pandas(case_ids_valid)\n\u001b[0;32m      5\u001b[0m base_test, X_test, y_test \u001b[38;5;241m=\u001b[39m from_polars_to_pandas(case_ids_test)\n",
      "Cell \u001b[1;32mIn[104], line 27\u001b[0m, in \u001b[0;36mfrom_polars_to_pandas\u001b[1;34m(case_ids)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_polars_to_pandas\u001b[39m(case_ids: pl\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Filtramos los datos por 'case_id' y seleccionamos las columnas 'case_id', 'WEEK_NUM' y 'target'.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Convertimos los datos a Pandas.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 27\u001b[0m         \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcase_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcase_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWEEK_NUM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_decision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# Filtramos los datos por 'case_id' y seleccionamos las columnas de predicción.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;66;03m# Convertimos los datos a Pandas.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         data\u001b[38;5;241m.\u001b[39mfilter(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mis_in(case_ids))[cols_pred]\u001b[38;5;241m.\u001b[39mto_pandas(),\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m# Filtramos los datos por 'case_id' y seleccionamos la columna 'target'.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# Convertimos los datos a Pandas.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         data\u001b[38;5;241m.\u001b[39mfilter(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mis_in(case_ids))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_decision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m     34\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\polars\\dataframe\\frame.py:1710\u001b[0m, in \u001b[0;36mDataFrame.to_pandas\u001b[1;34m(self, use_pyarrow_extension_array, **kwargs)\u001b[0m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Object \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes:\n\u001b[0;32m   1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_pandas_with_object_columns(\n\u001b[0;32m   1707\u001b[0m         use_pyarrow_extension_array\u001b[38;5;241m=\u001b[39muse_pyarrow_extension_array, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1708\u001b[0m     )\n\u001b[1;32m-> 1710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_pandas_without_object_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_pyarrow_extension_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pyarrow_extension_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\polars\\dataframe\\frame.py:1762\u001b[0m, in \u001b[0;36mDataFrame._to_pandas_without_object_columns\u001b[1;34m(self, df, use_pyarrow_extension_array, **kwargs)\u001b[0m\n\u001b[0;32m   1759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m   1761\u001b[0m record_batches \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_df\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m-> 1762\u001b[0m tbl \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_batches(record_batches)\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pyarrow_extension_array:\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tbl\u001b[38;5;241m.\u001b[39mto_pandas(\n\u001b[0;32m   1765\u001b[0m         self_destruct\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1766\u001b[0m         split_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1767\u001b[0m         types_mapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m pa_dtype: pd\u001b[38;5;241m.\u001b[39mArrowDtype(pa_dtype),\n\u001b[0;32m   1768\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1769\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\polars\\dependencies.py:97\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     95\u001b[0m pfx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mod_pfx\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpfx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m module to be installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: pa.Table requires 'pyarrow' module to be installed"
     ]
    }
   ],
   "source": [
    "# Usamos la función definida anteriormente para convertir los conjuntos de entrenamiento, validación y prueba a Pandas.\n",
    "!pip install pyarrow\n",
    "base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
    "base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
    "base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
    "\n",
    "# Convertimos las cadenas en los conjuntos de entrenamiento, validación y prueba.\n",
    "for df in [X_train, X_valid, X_test]:\n",
    "    df = convertir_cadenas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los archivos CSV y los cargamos en DataFrames de Polars.\n",
    "train_basetable = pl.read_csv(dataPath + \"csv_files/train/train_base.csv\")\n",
    "# Concatenamos los archivos CSV 'train_static_0_0.csv' y 'train_static_0_1.csv' verticalmente.\n",
    "train_static = pl.concat(\n",
    "    [\n",
    "        pl.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\").pipe(establecer_tipos_de_datos_tabla),\n",
    "        pl.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\").pipe(establecer_tipos_de_datos_tabla),\n",
    "    ],\n",
    "    how=\"vertical_relaxed\",\n",
    ")\n",
    "train_static_cb = pl.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_person_1 = pl.read_csv(dataPath + \"csv_files/train/train_person_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_credit_bureau_b_2 = pl.read_csv(dataPath + \"csv_files/train/train_credit_bureau_b_2.csv\").pipe(establecer_tipos_de_datos_tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valid\")\n",
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test\")\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se puede hacer con LightGBM pero, podemos hacerlo por random forest u otras opciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para subir el archivo al concurso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
