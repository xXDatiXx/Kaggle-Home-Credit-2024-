{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@misc{home-credit-credit-risk-model-stability,\\n    author = {Daniel Herman, Tomas Jelinek, Walter Reade, Maggie Demkin, Addison Howard},\\n    title = {Home Credit - Credit Risk Model Stability},\\n    publisher = {Kaggle},\\n    year = {2024},\\n    url = {https://kaggle.com/competitions/home-credit-credit-risk-model-stability}\\n}\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@misc{home-credit-credit-risk-model-stability,\n",
    "    author = {Daniel Herman, Tomas Jelinek, Walter Reade, Maggie Demkin, Addison Howard},\n",
    "    title = {Home Credit - Credit Risk Model Stability},\n",
    "    publisher = {Kaggle},\n",
    "    year = {2024},\n",
    "    url = {https://kaggle.com/competitions/home-credit-credit-risk-model-stability}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHome Credit - Credit Risk Model Stability\\nCreate a model measured against feature stability over time\\nThe goal of this competition is to predict which clients are more likely to default on their loans. \\nThe evaluation will favor solutions that are stable over time.\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Home Credit - Credit Risk Model Stability\n",
    "Create a model measured against feature stability over time\n",
    "The goal of this competition is to predict which clients are more likely to default on their loans. \n",
    "The evaluation will favor solutions that are stable over time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLack of credit history could indicate youth or a preference for using cash.\\nTraditional data scarcity often leads to loan application rejections.\\nData science is essential in determining loan repayment capabilities, potentially making loans more accessible to those in need.\\nConsumer finance providers utilize various statistical and machine learning methods in scorecards to assess loan risk.\\nScorecards must be updated regularly due to the constant change in client behavior, making the process time-consuming.\\nThe stability of a scorecard is crucial; a performance decline could mean issuing loans to higher-risk clients.\\nEarly detection of loan repayment issues is challenging and usually visible only after loan due dates.\\nBalancing the stability and performance of scorecards is necessary before their implementation.\\nHome Credit, established in 1997, aims to offer responsible lending to people without significant credit history.\\nHome Credit's efforts in financial inclusion have previously been showcased in a Kaggle competition.\\nImproving scorecard assessments could help consumer finance providers accept more loan applications, enhancing financial access for historically underserved groups.\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lack of credit history could indicate youth or a preference for using cash.\n",
    "Traditional data scarcity often leads to loan application rejections.\n",
    "Data science is essential in determining loan repayment capabilities, potentially making loans more accessible to those in need.\n",
    "Consumer finance providers utilize various statistical and machine learning methods in scorecards to assess loan risk.\n",
    "Scorecards must be updated regularly due to the constant change in client behavior, making the process time-consuming.\n",
    "The stability of a scorecard is crucial; a performance decline could mean issuing loans to higher-risk clients.\n",
    "Early detection of loan repayment issues is challenging and usually visible only after loan due dates.\n",
    "Balancing the stability and performance of scorecards is necessary before their implementation.\n",
    "Home Credit, established in 1997, aims to offer responsible lending to people without significant credit history.\n",
    "Home Credit's efforts in financial inclusion have previously been showcased in a Kaggle competition.\n",
    "Improving scorecard assessments could help consumer finance providers accept more loan applications, enhancing financial access for historically underserved groups.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor each case_id in the test set, you must predict a probability for the target score. The file should contain a header and have the following format:\\n\\ncase_id,score\\n57543,0.1\\n57544,0.9\\n57545,0.5\\netc.\\n\\n\\nSubmission file must be named submission.csv\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each case_id in the test set, you must predict a probability for the target score. The file should contain a header and have the following format:\n",
    "\n",
    "case_id,score\n",
    "57543,0.1\n",
    "57544,0.9\n",
    "57545,0.5\n",
    "etc.\n",
    "\n",
    "\n",
    "Submission file must be named submission.csv\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n predicting default of clients based on internal and external information that are available for each client. Scoring is \\n performed using custom metric that not only evaluates the AUC of predictions but also considers the stability of predictions \\n model across the data range of the test set.\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " predicting default of clients based on internal and external information that are available for each client. Scoring is \n",
    " performed using custom metric that not only evaluates the AUC of predictions but also considers the stability of predictions \n",
    " model across the data range of the test set.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl #Libreria para manipulacion de datos, fast wrote in rust https://docs.pola.rs \n",
    "import numpy as np #Libreria para manipulacion de datos \n",
    "import pandas as pd #Libreria para manipulacion de datos \n",
    "import lightgbm as lgb #Libreria para modelos de machine learning,  is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cambia la ruta de acuerdo a la ubicacion de los datos\n",
    "dataPath = \"C:/Users/diego/Documents/UP/Negocios/DataProyecto/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establecer_tipos_de_datos_tabla(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # implementar aquí todos los tipos de datos deseados para las tablas\n",
    "    # lo siguiente es solo un ejemplo\n",
    "    for columna in df.columns:\n",
    "        # la última letra del nombre de la columna te ayudará a determinar el tipo\n",
    "        if columna[-1] in (\"P\", \"A\"):\n",
    "            df = df.with_columns(pl.col(columna).cast(pl.Float64).alias(columna)) #Pasa las columnas a float64\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_cadenas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for columna in df.columns:  \n",
    "        if df[columna].dtype.name in ['object', 'string']:\n",
    "            df[columna] = df[columna].astype(\"string\").astype('category')\n",
    "            categorias_actuales = df[columna].cat.categories\n",
    "            nuevas_categorias = categorias_actuales.to_list() + [\"Desconocido\"]\n",
    "            nuevo_tipo = pd.CategoricalDtype(categories=nuevas_categorias, ordered=True)\n",
    "            df[columna] = df[columna].astype(nuevo_tipo)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_base = dataPath + \"csv_files/train/\"\n",
    "\n",
    "# Leer el archivo base de entrenamiento\n",
    "tabla_base_entrenamiento = pl.read_csv(ruta_base + \"train_base.csv\")\n",
    "\n",
    "# Leer y concatenar los archivos estáticos de entrenamiento\n",
    "archivo_estatico_1 = pl.read_csv(ruta_base + \"train_static_0_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "archivo_estatico_2 = pl.read_csv(ruta_base + \"train_static_0_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_static = pl.concat([archivo_estatico_1, archivo_estatico_2], how=\"vertical_relaxed\")\n",
    "\n",
    "# Leer otros archivos de entrenamiento\n",
    "entrenamiento_estatico_cb = pl.read_csv(ruta_base + \"train_static_cb_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_person_1 = pl.read_csv(ruta_base + \"train_person_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_credit_bureau_b_2 = pl.read_csv(ruta_base + \"train_credit_bureau_b_2.csv\").pipe(establecer_tipos_de_datos_tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_base_prueba = dataPath + \"csv_files/test/\"\n",
    "\n",
    "# Leer el archivo base de prueba\n",
    "train_basetable = pl.read_csv(ruta_base_prueba + \"test_base.csv\")\n",
    "\n",
    "# Leer y concatenar los archivos estáticos de prueba\n",
    "archivo_estatico_prueba_1 = pl.read_csv(ruta_base_prueba + \"test_static_0_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "archivo_estatico_prueba_2 = pl.read_csv(ruta_base_prueba + \"test_static_0_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "archivo_estatico_prueba_3 = pl.read_csv(ruta_base_prueba + \"test_static_0_2.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "prueba_estatica = pl.concat([archivo_estatico_prueba_1, archivo_estatico_prueba_2, archivo_estatico_prueba_3], how=\"vertical_relaxed\")\n",
    "\n",
    "# Leer otros archivos de prueba\n",
    "train_static_cb = pl.read_csv(ruta_base_prueba + \"test_static_cb_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "test_person_1 = pl.read_csv(ruta_base_prueba + \"test_person_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "test_credit_bureau_b_2 = pl.read_csv(ruta_base_prueba + \"test_credit_bureau_b_2.csv\").pipe(establecer_tipos_de_datos_tabla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A']\n",
      "['description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n"
     ]
    }
   ],
   "source": [
    "# Necesitamos usar funciones de agregación en tablas con profundidad > 1, es decir, tablas que contienen la columna num_group1 o\n",
    "# también la columna num_group2.\n",
    "caracteristicas_persona_1 = train_person_1.group_by(\"case_id\").agg(\n",
    "    # Obtenemos el valor máximo de la columna 'mainoccupationinc_384A' para cada 'case_id'\n",
    "    pl.col(\"mainoccupationinc_384A\").max().alias(\"maximo_mainoccupationinc_384A\"),\n",
    "    # Verificamos si alguna vez 'incometype_1044T' es 'SELFEMPLOYED' para cada 'case_id'\n",
    "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"alguna_vez_selfemployed\")\n",
    ")\n",
    "\n",
    "# Aquí num_group1=0 tiene un significado especial, es la persona que solicitó el préstamo.\n",
    "caracteristicas_persona_2 = train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
    "    # Filtramos las filas donde 'num_group1' es 0\n",
    "    pl.col(\"num_group1\") == 0\n",
    "# Eliminamos la columna 'num_group1' y renombramos la columna 'housetype_905L' a 'tipo_casa_persona'\n",
    ").drop(\"num_group1\").rename({\"housetype_905L\": \"tipo_casa_persona\"})\n",
    "\n",
    "# Aquí tenemos num_goup1 y num_group2, por lo que necesitamos agregar de nuevo.\n",
    "caracteristicas_credito_b_2 = train_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
    "    # Obtenemos el valor máximo de la columna 'pmts_pmtsoverdue_635A' para cada 'case_id'\n",
    "    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"maximo_pmts_pmtsoverdue_635A\"),\n",
    "    # Verificamos si alguna vez 'pmts_dpdvalue_108P' es mayor que 31 para cada 'case_id'\n",
    "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"alguna_vez_pmts_dpdvalue_108P_mayor_31\")\n",
    ")\n",
    "\n",
    "# Procesaremos en este ejemplo solo las columnas de tipo A y M, por lo que necesitamos seleccionarlas.\n",
    "columnas_estaticas_seleccionadas = [col for col in train_static.columns if col[-1] in (\"A\", \"M\")]\n",
    "print(columnas_estaticas_seleccionadas)\n",
    "\n",
    "columnas_estaticas_cb_seleccionadas = [col for col in train_static_cb.columns if col[-1] in (\"A\", \"M\")]\n",
    "print(columnas_estaticas_cb_seleccionadas)\n",
    "\n",
    "# Unimos todas las tablas.\n",
    "data = train_basetable.join(\n",
    "    train_static.select([\"case_id\"]+columnas_estaticas_seleccionadas), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    train_static_cb.select([\"case_id\"]+columnas_estaticas_cb_seleccionadas), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    caracteristicas_persona_1, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    caracteristicas_persona_2, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    caracteristicas_credito_b_2, how=\"left\", on=\"case_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos usar funciones de agregación en tablas con profundidad > 1, es decir, tablas que contienen la columna num_group1 o\n",
    "# también la columna num_group2.\n",
    "test_person_1_feats_1 = test_person_1.group_by(\"case_id\").agg(\n",
    "    # Obtenemos el valor máximo de la columna 'mainoccupationinc_384A' para cada 'case_id'\n",
    "    pl.col(\"mainoccupationinc_384A\").max().alias(\"maximo_mainoccupationinc_384A\"),\n",
    "    # Verificamos si alguna vez 'incometype_1044T' es 'SELFEMPLOYED' para cada 'case_id'\n",
    "    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"alguna_vez_selfemployed\")\n",
    ")\n",
    "\n",
    "# Aquí num_group1=0 tiene un significado especial, es la persona que solicitó el préstamo.\n",
    "test_person_1_feats_2 = test_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
    "    # Filtramos las filas donde 'num_group1' es 0\n",
    "    pl.col(\"num_group1\") == 0\n",
    "# Eliminamos la columna 'num_group1' y renombramos la columna 'housetype_905L' a 'tipo_casa_persona'\n",
    ").drop(\"num_group1\").rename({\"housetype_905L\": \"tipo_casa_persona\"})\n",
    "\n",
    "# Aquí tenemos num_goup1 y num_group2, por lo que necesitamos agregar de nuevo.\n",
    "test_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
    "    # Obtenemos el valor máximo de la columna 'pmts_pmtsoverdue_635A' para cada 'case_id'\n",
    "    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"maximo_pmts_pmtsoverdue_635A\"),\n",
    "    # Verificamos si alguna vez 'pmts_dpdvalue_108P' es mayor que 31 para cada 'case_id'\n",
    "    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"alguna_vez_pmts_dpdvalue_108P_mayor_31\")\n",
    ")\n",
    "\n",
    "# Unimos todas las tablas.\n",
    "data_submission = train_basetable.join(\n",
    "    prueba_estatica.select([\"case_id\"]+columnas_estaticas_seleccionadas), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    train_static_cb.select([\"case_id\"]+columnas_estaticas_cb_seleccionadas), how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
    ").join(\n",
    "    test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use `__getitem__` on Series of dtype Int64 with argument (array([0, 3, 1, 7, 8, 5]), Ellipsis) of type 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m case_ids \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Luego, dividimos los 'case_id' en conjuntos de entrenamiento y prueba.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m case_ids_train, case_ids_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Dividimos el conjunto de prueba en conjuntos de validación y prueba.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m case_ids_valid, case_ids_test \u001b[38;5;241m=\u001b[39m train_test_split(case_ids_test, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2683\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2679\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2681\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m-> 2683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2686\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2685\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2679\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2681\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2684\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2685\u001b[0m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2686\u001b[0m     )\n\u001b[0;32m   2687\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\__init__.py:386\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _polars_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\__init__.py:196\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    195\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\polars\\series\\series.py:1325\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_with_series(idx_series)\n\u001b[0;32m   1321\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot use `__getitem__` on Series of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(item)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m )\n\u001b[1;32m-> 1325\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot use `__getitem__` on Series of dtype Int64 with argument (array([0, 3, 1, 7, 8, 5]), Ellipsis) of type 'tuple'"
     ]
    }
   ],
   "source": [
    "# Primero, obtenemos los 'case_id' únicos y los mezclamos.\n",
    "case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
    "\n",
    "# Luego, dividimos los 'case_id' en conjuntos de entrenamiento y prueba.\n",
    "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
    "# Dividimos el conjunto de prueba en conjuntos de validación y prueba.\n",
    "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
    "\n",
    "# Creamos una lista vacía para almacenar las columnas de predicción.\n",
    "cols_pred = []\n",
    "# Recorremos todas las columnas en los datos.\n",
    "for col in data.columns:\n",
    "    # Si el último carácter de la columna es una letra mayúscula y el resto de la columna es minúscula,\n",
    "    # añadimos la columna a la lista de columnas de predicción.\n",
    "    if col[-1].isupper() and col[:-1].islower():\n",
    "        cols_pred.append(col)\n",
    "\n",
    "# Imprimimos las columnas de predicción.\n",
    "print(cols_pred)\n",
    "\n",
    "# Definimos una función para convertir los datos de Polars a Pandas.\n",
    "def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Filtramos los datos por 'case_id' y seleccionamos las columnas 'case_id', 'WEEK_NUM' y 'target'.\n",
    "    # Convertimos los datos a Pandas.\n",
    "    return (\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
    "        # Filtramos los datos por 'case_id' y seleccionamos las columnas de predicción.\n",
    "        # Convertimos los datos a Pandas.\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
    "        # Filtramos los datos por 'case_id' y seleccionamos la columna 'target'.\n",
    "        # Convertimos los datos a Pandas.\n",
    "        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
    "    )\n",
    "\n",
    "# Usamos la función definida anteriormente para convertir los conjuntos de entrenamiento, validación y prueba a Pandas.\n",
    "base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
    "base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
    "base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
    "\n",
    "# Convertimos las cadenas en los conjuntos de entrenamiento, validación y prueba.\n",
    "for df in [X_train, X_valid, X_test]:\n",
    "    df = convertir_cadenas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los archivos CSV y los cargamos en DataFrames de Polars.\n",
    "train_basetable = pl.read_csv(dataPath + \"csv_files/train/train_base.csv\")\n",
    "# Concatenamos los archivos CSV 'train_static_0_0.csv' y 'train_static_0_1.csv' verticalmente.\n",
    "train_static = pl.concat(\n",
    "    [\n",
    "        pl.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\").pipe(establecer_tipos_de_datos_tabla),\n",
    "        pl.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\").pipe(establecer_tipos_de_datos_tabla),\n",
    "    ],\n",
    "    how=\"vertical_relaxed\",\n",
    ")\n",
    "train_static_cb = pl.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_person_1 = pl.read_csv(dataPath + \"csv_files/train/train_person_1.csv\").pipe(establecer_tipos_de_datos_tabla)\n",
    "train_credit_bureau_b_2 = pl.read_csv(dataPath + \"csv_files/train/train_credit_bureau_b_2.csv\").pipe(establecer_tipos_de_datos_tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valid\")\n",
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test\")\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se puede hacer con LightGBM pero, podemos hacerlo por random forest u otras opciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para subir el archivo al concurso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
